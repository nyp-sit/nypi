{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/nypi/blob/main/day4am/stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8Ftj8GOHMz"
      },
      "source": [
        "# Stable Diffusion with ðŸ¤— Diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVQQbnqDOHM8"
      },
      "source": [
        "This notebook introduces Stable Diffusion, the highest-quality open source text to image model as of now. It's also small enough to run in consumer GPUs rather than in a datacenter. We use the ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers), which is currently the recommended library for using diffusion models.\n",
        "\n",
        "This notebook shows what Stable Diffusion can do and a glimpse of its main components. We will not cover the training and fine-tuning of Stable Diffusion, a process that will take significantly more time and more compute resources.\n",
        "\n",
        "*Acknowledgement: This notebook is adapted from the FastAI diffusion course*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FdTo8koOHM9"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq diffusers transformers fastcore gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUdZR9aNOHM-"
      },
      "source": [
        "## Using Stable Diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8hNFo1WOHM-"
      },
      "source": [
        "To run Stable Diffusion on your computer you have to accept the model license. It's an open CreativeML OpenRail-M license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) provides more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Eesow2-OHM-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "from fastcore.all import concat\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue-3riq8OHM-"
      },
      "source": [
        "### Stable Diffusion Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IpKrApLOHM_"
      },
      "source": [
        "[`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline) is an end-to-end [diffusion inference pipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion) that allows you to start generating images with just a few lines of code. Many Hugging Face libraries (along with other libraries such as scikit-learn) use the concept of a \"pipeline\" to indicate a sequence of steps that when combined complete some task. We'll look at the individual steps of the pipeline later -- for now though, let's just use it to see what it can do.\n",
        "\n",
        "When we say \"inference\" we're referring to using an existing model to generate samples (in this case, images), as opposed to \"training\" (or fine-tuning) models using new data.\n",
        "\n",
        "We use [`from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained) to create the pipeline and download the pretrained weights. We indicate that we want to use the `fp16` (half-precision) version of the weights, and we tell `diffusers` to expect the weights in that format. This allows us to perform much faster inference with almost no discernible difference in quality. The string passed to `from_pretrained` in this case (`stabilityai/stable-diffusion-2-1`) is the repo id of a pretrained pipeline hosted on [Hugging Face Hub](https://huggingface.co/models); it can also be a path to a directory containing pipeline weights. The weights for all the models in the pipeline will be downloaded and cached the first time you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TakyTJ4OHM_"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8w9xn56OHNA"
      },
      "source": [
        "The weights are cached in your home directory by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaOgMca6OHNA"
      },
      "outputs": [],
      "source": [
        "!ls ~/.cache/huggingface/hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27XK8oTnOHNA"
      },
      "source": [
        "We are now ready to use the pipeline to start creating images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGXdOEeVOHNB"
      },
      "outputs": [],
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSTDHOh-OHNB"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIEifNoMOHNC"
      },
      "source": [
        "You will have noticed that running the pipeline shows a progress bar with a certain number of steps. This is because Stable Diffusion is based on a progressive denoising algorithm that is able to create a convincing image starting from pure random noise. Models in this family are known as _diffusion models_. Here's an example of the process (from random noise at top to progressively improved images towards the bottom) of a model drawing handwritten digits.\n",
        "\n",
        "![digit_diffusion](https://raw.githubusercontent.com/nyp-sit/nypi/main/day4/digit_diffusion.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlqR0TPUOHNC"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt, num_inference_steps=3).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0_F7XVYOHNC"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt, num_inference_steps=16).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z4FPgZ2OHND"
      },
      "source": [
        "### Classifier-Free Guidance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9ckSJj-OHND"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    w,h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te4i8GsxOHND"
      },
      "source": [
        "_Classifier-Free Guidance_ is a method to increase the adherence of the output to the conditioning signal we used (the text).\n",
        "\n",
        "Roughly speaking, the larger the guidance the more the model tries to represent the text prompt. However, large values tend to produce less diversity. The default is `7.5`, which represents a good compromise between variety and fidelity. This [blog post](https://benanne.github.io/2022/05/26/guidance.html) goes into deeper details on how it works.\n",
        "\n",
        "We can generate multiple images for the same prompt by simply passing a list of prompts instead of a string."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = [pipe(prompt, guidance_scale=g).images[0] for g in [1.1, 3, 7, 14]]"
      ],
      "metadata": {
        "id": "2wDMcgGmKou9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oGJbnO1OHNE"
      },
      "outputs": [],
      "source": [
        "image_grid(images, rows=1, cols=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H45YRVpvOHNE"
      },
      "source": [
        "### Negative prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TWAsdRXOHNE"
      },
      "source": [
        "_Negative prompting_ refers to the use of another prompt (instead of a completely unconditioned generation), and scaling the difference between generations of that prompt and the conditioned generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0ke4IWNOHNE"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "prompt = \"Labrador wearing a hat in the style of Vermeer\"\n",
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCT0rE7yOHNE"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt, negative_prompt=\"yellow color\").images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veG-8EuwOHNF"
      },
      "source": [
        "By using the negative prompt we move more towards the direction of the positive prompt, effectively reducing the importance of the negative prompt in our composition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTP8uHpGOHNF"
      },
      "source": [
        "### Image to Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOfvQmvoOHNF"
      },
      "source": [
        "Even though Stable Diffusion was trained to generate images, and optionally drive the generation using text conditioning, we can use the raw image diffusion process for other tasks.\n",
        "\n",
        "For example, instead of starting from pure noise, we can start from an image an add a certain amount of noise to it. We are replacing the initial steps of the denoising and pretending our image is what the algorithm came up with. Then we continue the diffusion process from that state as usual.\n",
        "\n",
        "This usually preserves the composition although details may change a lot. It's great for sketches!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMCNwkCvOHNF"
      },
      "source": [
        "These operations (provide an initial image, add some noise to it and run diffusion from there) can be automatically performed by a special image to image pipeline: `StableDiffusionDepth2ImgPipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-BsWwWkOHNG"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionDepth2ImgPipeline\n",
        "from fastdownload import FastDownload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttKjDzn1OHNG"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-depth\",\n",
        "    revision=\"fp16\",\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dsNdhlwOHNG"
      },
      "outputs": [],
      "source": [
        "p = FastDownload().download('https://raw.githubusercontent.com/nyp-sit/nypi/main/day4am/lala-land.png')\n",
        "init_image = Image.open(p).convert(\"RGB\")\n",
        "init_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8aGvTpMOHNK"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(2000)\n",
        "prompt = \"Two men are wrestling\"\n",
        "# negative_prompt = ''\n",
        "strength = 0.85\n",
        "images = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=strength).images\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_grid(images, rows=1, cols=3)"
      ],
      "metadata": {
        "id": "X65xNZxxP1sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In-painting\n",
        "\n",
        "Inpainting is a process where missing parts of an artwork are filled in to present a complete image."
      ],
      "metadata": {
        "id": "NEkJMvk5Ub25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "\n",
        "model_path = \"stabilityai/stable-diffusion-2-inpainting\"\n",
        "\n",
        "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        ").to('cuda')"
      ],
      "metadata": {
        "id": "DT6tKEjrVBhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://raw.githubusercontent.com/nyp-sit/nypi/main/day4am/dog_on_bench.png\"\n",
        "mask_url = \"https://raw.githubusercontent.com/nyp-sit/nypi/main/day4am/dog_on_bench_mask.png\""
      ],
      "metadata": {
        "id": "YOqsjFRRVO3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = FastDownload().download(img_url)\n",
        "image = Image.open(p).convert('RGB').resize((512,512))\n",
        "image"
      ],
      "metadata": {
        "id": "W6KWQRu3WBJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mask that we download represent the part that is removed (missing). We will later get the diffusion model to fill in content based on our text prompt."
      ],
      "metadata": {
        "id": "3HrlGP2aagOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = FastDownload().download(mask_url)\n",
        "mask_image = Image.open(p).resize((512, 512))\n",
        "mask_image"
      ],
      "metadata": {
        "id": "wqZAdbKUWk_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Cat sitting on the bench.\"\n",
        "\n",
        "guidance_scale=7.5\n",
        "num_samples = 3\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(100) # change the seed to get different results\n",
        "\n",
        "images = pipe(\n",
        "    prompt=prompt,\n",
        "    image=image,\n",
        "    mask_image=mask_image,\n",
        "    guidance_scale=guidance_scale,\n",
        "    generator=generator,\n",
        "    num_inference_steps=80,\n",
        "    num_images_per_prompt=num_samples,\n",
        ").images"
      ],
      "metadata": {
        "id": "UGsPGOPXW1c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert initial image in the list so we can compare side by side\n",
        "images.insert(0, image)\n",
        "image_grid(images, 1, num_samples + 1)"
      ],
      "metadata": {
        "id": "hzt0lySXXCbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio demo of In-painting\n"
      ],
      "metadata": {
        "id": "MW0OQziLXMMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the codes below, we build an easy to use Gradio app to create your own mask based on your own custom image, and using the created mask, we will do the in-painting as before."
      ],
      "metadata": {
        "id": "wzTSuab-a0pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(dict, prompt):\n",
        "\n",
        "    guidance_scale=7.5\n",
        "    image =  dict['image'].convert(\"RGB\").resize((512, 512))\n",
        "    mask_image = dict['mask'].convert(\"RGB\").resize((512, 512))\n",
        "    images = pipe(\n",
        "        prompt=prompt,\n",
        "        image=image,\n",
        "        mask_image=mask_image,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=generator,\n",
        "        num_inference_steps=80).images\n",
        "\n",
        "    return(images[0])"
      ],
      "metadata": {
        "id": "swnR1-euXFO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(\n",
        "    predict,\n",
        "    title = 'Stable Diffusion In-Painting',\n",
        "    inputs=[\n",
        "        gr.Image(source = 'upload', tool = 'sketch', type = 'pil'),\n",
        "        gr.Textbox(label = 'prompt')\n",
        "    ],\n",
        "    outputs = [\n",
        "        gr.Image()\n",
        "        ]\n",
        ").launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "KhgYWQhfYYnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVz74jmXYcHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}